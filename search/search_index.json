{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Read Latest Documentation - Browse GitHub Code Repository hypothesis-auto is an extension for the Hypothesis project that enables fully automatic tests for type annotated functions. Key Features: Type Annotation Powered : Utilize your function's existing type annotations to build dozens of test cases automatically. Low Barrier : Start utilizing property-based testing in the lowest barrier way possible. Just run auto_test(FUNCTION) to run dozens of test. pytest Compatible : Built-in compatibility with the popular pytest testing framework. This means that you can turn your automatically generated tests into individual pytest test cases with one line. Scales Up : As you find your self needing to customize your auto_test cases, you can easily utilize all the features of Hypothesis , including custom strategies per a parameter. Installation: To get started - install hypothesis-auto into your projects virtual environment: pip3 install hypothesis-auto OR poetry add hypothesis-auto OR pipenv install hypothesis-auto Usage Examples: Warning In old usage examples you will see _ prefixed parameters like _auto_verify= . This was done to avoid conflicting with existing function parameters. Based on community feedback the project switched to _ suffixes, such as auto_verify_= to keep the likely hood of conflicting low while avoiding the connotation of private parameters. Framework independent usage Basic auto_test usage: from hypothesis_auto import auto_test def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 auto_test ( add ) # 50 property based scenarios are generated and ran against add auto_test ( add , auto_runs_ = 1_000 ) # Let's make that 1,000 Adding an allowed exception: from hypothesis_auto import auto_test def divide ( number_1 : int , number_2 : int ) -> int : return number_1 / number_2 auto_test ( divide ) -> 1012 raise the_error_hypothesis_found 1013 1014 for attrib in dir ( test ): < ipython - input - 2 - 65 a3aa66e9f9 > in divide ( number_1 , number_2 ) 1 def divide ( number_1 : int , number_2 : int ) -> int : ----> 2 return number_1 / number_2 3 0 / 0 ZeroDivisionError : division by zero auto_test ( divide , auto_allow_exceptions_ = ( ZeroDivisionError , )) Using auto_test with a custom verification method: from hypothesis_auto import Scenario , auto_test def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 def my_custom_verifier ( scenario : Scenario ): if scenario . kwargs [ \"number_1\" ] > 0 and scenario . kwargs [ \"number_2\" ] > 0 : assert scenario . result > scenario . kwargs [ \"number_1\" ] assert scenario . result > scenario . kwargs [ \"number_1\" ] elif scenario . kwargs [ \"number_1\" ] < 0 and scenario . kwargs [ \"number_2\" ] < 0 : assert scenario . result < scenario . kwargs [ \"number_1\" ] assert scenario . result < scenario . kwargs [ \"number_1\" ] else : assert scenario . result >= min ( scenario . kwargs . values ()) assert scenario . result <= max ( scenario . kwargs . values ()) auto_test ( add , auto_verify_ = my_custom_verifier ) Custom verification methods should take a single Scenario and raise an exception to signify errors. For the full set of parameters, you can pass into auto_test see its API reference documentation . pytest usage Using auto_pytest_magic to auto-generate dozens of pytest test cases: from hypothesis_auto import auto_pytest_magic def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 auto_pytest_magic ( add ) Using auto_pytest to run dozens of test case within a temporary directory: from hypothesis_auto import auto_pytest def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 @auto_pytest () def test_add ( test_case , tmpdir ): tmpdir . mkdir () . chdir () test_case () Using auto_pytest_magic with a custom verification method: from hypothesis_auto import Scenario , auto_pytest def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 def my_custom_verifier ( scenario : Scenario ): if scenario . kwargs [ \"number_1\" ] > 0 and scenario . kwargs [ \"number_2\" ] > 0 : assert scenario . result > scenario . kwargs [ \"number_1\" ] assert scenario . result > scenario . kwargs [ \"number_1\" ] elif scenario . kwargs [ \"number_1\" ] < 0 and scenario . kwargs [ \"number_2\" ] < 0 : assert scenario . result < scenario . kwargs [ \"number_1\" ] assert scenario . result < scenario . kwargs [ \"number_1\" ] else : assert scenario . result >= min ( scenario . kwargs . values ()) assert scenario . result <= max ( scenario . kwargs . values ()) auto_pytest_magic ( add , auto_verify_ = my_custom_verifier ) Custom verification methods should take a single Scenario and raise an exception to signify errors. For the full reference of the pytest integration API see the API reference documentation . Why Create hypothesis-auto? I wanted a no/low resistance way to start incorporating property-based tests across my projects. Such a solution that also encouraged the use of type hints was a win/win for me. I hope you too find hypothesis-auto useful! ~Timothy Crosley","title":"Home"},{"location":"#installation","text":"To get started - install hypothesis-auto into your projects virtual environment: pip3 install hypothesis-auto OR poetry add hypothesis-auto OR pipenv install hypothesis-auto","title":"Installation:"},{"location":"#usage-examples","text":"Warning In old usage examples you will see _ prefixed parameters like _auto_verify= . This was done to avoid conflicting with existing function parameters. Based on community feedback the project switched to _ suffixes, such as auto_verify_= to keep the likely hood of conflicting low while avoiding the connotation of private parameters.","title":"Usage Examples:"},{"location":"#framework-independent-usage","text":"","title":"Framework independent usage"},{"location":"#basic-auto_test-usage","text":"from hypothesis_auto import auto_test def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 auto_test ( add ) # 50 property based scenarios are generated and ran against add auto_test ( add , auto_runs_ = 1_000 ) # Let's make that 1,000","title":"Basic auto_test usage:"},{"location":"#adding-an-allowed-exception","text":"from hypothesis_auto import auto_test def divide ( number_1 : int , number_2 : int ) -> int : return number_1 / number_2 auto_test ( divide ) -> 1012 raise the_error_hypothesis_found 1013 1014 for attrib in dir ( test ): < ipython - input - 2 - 65 a3aa66e9f9 > in divide ( number_1 , number_2 ) 1 def divide ( number_1 : int , number_2 : int ) -> int : ----> 2 return number_1 / number_2 3 0 / 0 ZeroDivisionError : division by zero auto_test ( divide , auto_allow_exceptions_ = ( ZeroDivisionError , ))","title":"Adding an allowed exception:"},{"location":"#using-auto_test-with-a-custom-verification-method","text":"from hypothesis_auto import Scenario , auto_test def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 def my_custom_verifier ( scenario : Scenario ): if scenario . kwargs [ \"number_1\" ] > 0 and scenario . kwargs [ \"number_2\" ] > 0 : assert scenario . result > scenario . kwargs [ \"number_1\" ] assert scenario . result > scenario . kwargs [ \"number_1\" ] elif scenario . kwargs [ \"number_1\" ] < 0 and scenario . kwargs [ \"number_2\" ] < 0 : assert scenario . result < scenario . kwargs [ \"number_1\" ] assert scenario . result < scenario . kwargs [ \"number_1\" ] else : assert scenario . result >= min ( scenario . kwargs . values ()) assert scenario . result <= max ( scenario . kwargs . values ()) auto_test ( add , auto_verify_ = my_custom_verifier ) Custom verification methods should take a single Scenario and raise an exception to signify errors. For the full set of parameters, you can pass into auto_test see its API reference documentation .","title":"Using auto_test with a custom verification method:"},{"location":"#pytest-usage","text":"","title":"pytest usage"},{"location":"#using-auto_pytest_magic-to-auto-generate-dozens-of-pytest-test-cases","text":"from hypothesis_auto import auto_pytest_magic def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 auto_pytest_magic ( add )","title":"Using auto_pytest_magic to auto-generate dozens of pytest test cases:"},{"location":"#using-auto_pytest-to-run-dozens-of-test-case-within-a-temporary-directory","text":"from hypothesis_auto import auto_pytest def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 @auto_pytest () def test_add ( test_case , tmpdir ): tmpdir . mkdir () . chdir () test_case ()","title":"Using auto_pytest to run dozens of test case within a temporary directory:"},{"location":"#using-auto_pytest_magic-with-a-custom-verification-method","text":"from hypothesis_auto import Scenario , auto_pytest def add ( number_1 : int , number_2 : int = 1 ) -> int : return number_1 + number_2 def my_custom_verifier ( scenario : Scenario ): if scenario . kwargs [ \"number_1\" ] > 0 and scenario . kwargs [ \"number_2\" ] > 0 : assert scenario . result > scenario . kwargs [ \"number_1\" ] assert scenario . result > scenario . kwargs [ \"number_1\" ] elif scenario . kwargs [ \"number_1\" ] < 0 and scenario . kwargs [ \"number_2\" ] < 0 : assert scenario . result < scenario . kwargs [ \"number_1\" ] assert scenario . result < scenario . kwargs [ \"number_1\" ] else : assert scenario . result >= min ( scenario . kwargs . values ()) assert scenario . result <= max ( scenario . kwargs . values ()) auto_pytest_magic ( add , auto_verify_ = my_custom_verifier ) Custom verification methods should take a single Scenario and raise an exception to signify errors. For the full reference of the pytest integration API see the API reference documentation .","title":"Using auto_pytest_magic with a custom verification method:"},{"location":"#why-create-hypothesis-auto","text":"I wanted a no/low resistance way to start incorporating property-based tests across my projects. Such a solution that also encouraged the use of type hints was a win/win for me. I hope you too find hypothesis-auto useful! ~Timothy Crosley","title":"Why Create hypothesis-auto?"},{"location":"CHANGELOG/","text":"Install the latest To install the latest version of hypothesis-auto simply run: pip3 install hypothesis-auto OR poetry add hypothesis-auto OR pipenv install hypothesis-auto Changelog 1.1.1 - 19 September 2019 Fixed Issue #5 : typo in Scenario class name 1.1.0 - 18 September 2019 Danger This release contains breaking changes. In particular to how parameters are passed in to testing functions. _prefixing for parameters was replaced with suffixing_ to avoid private implication while still avoiding parameter name conflicts. Custom auto_verify_ function now must take a single Scenario object. 1.0.0 - 17 September 2019 Initial Release.","title":"Changelog"},{"location":"CHANGELOG/#install-the-latest","text":"To install the latest version of hypothesis-auto simply run: pip3 install hypothesis-auto OR poetry add hypothesis-auto OR pipenv install hypothesis-auto","title":"Install the latest"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#111-19-september-2019","text":"Fixed Issue #5 : typo in Scenario class name","title":"1.1.1 - 19 September 2019"},{"location":"CHANGELOG/#110-18-september-2019","text":"Danger This release contains breaking changes. In particular to how parameters are passed in to testing functions. _prefixing for parameters was replaced with suffixing_ to avoid private implication while still avoiding parameter name conflicts. Custom auto_verify_ function now must take a single Scenario object.","title":"1.1.0 - 18 September 2019"},{"location":"CHANGELOG/#100-17-september-2019","text":"Initial Release.","title":"1.0.0 - 17 September 2019"},{"location":"docs/contributing/1.-contributing-guide/","text":"Contributing to hypothesis-auto Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place. Getting hypothesis-auto set up for local development The first step when contributing to any project is getting it set up on your local machine. hypothesis-auto aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ poetry bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/$GITHUB_ACCOUNT/hypothesis-auto.git `cd hypothesis-auto poetry install Making a contribution Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main hypothesis-auto project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :). Thank you! I can not tell you how thankful I am for the hard work done by hypothesis-auto contributors like you . Thank you! ~Timothy Crosley","title":"1. Contributing Guide"},{"location":"docs/contributing/1.-contributing-guide/#contributing-to-hypothesis-auto","text":"Looking for a useful open source project to contribute to? Want your contributions to be warmly welcomed and acknowledged? Welcome! You have found the right place.","title":"Contributing to hypothesis-auto"},{"location":"docs/contributing/1.-contributing-guide/#getting-hypothesis-auto-set-up-for-local-development","text":"The first step when contributing to any project is getting it set up on your local machine. hypothesis-auto aims to make this as simple as possible. Account Requirements: A valid GitHub account Base System Requirements: Python3.6+ poetry bash or a bash compatible shell (should be auto-installed on Linux / Mac) Once you have verified that you system matches the base requirements you can start to get the project working by following these steps: Fork the project on GitHub . Clone your fork to your local file system: git clone https://github.com/$GITHUB_ACCOUNT/hypothesis-auto.git `cd hypothesis-auto poetry install","title":"Getting hypothesis-auto set up for local development"},{"location":"docs/contributing/1.-contributing-guide/#making-a-contribution","text":"Congrats! You're now ready to make a contribution! Use the following as a guide to help you reach a successful pull-request: Check the issues page on GitHub to see if the task you want to complete is listed there. If it's listed there, write a comment letting others know you are working on it. If it's not listed in GitHub issues, go ahead and log a new issue. Then add a comment letting everyone know you have it under control. If you're not sure if it's something that is good for the main hypothesis-auto project and want immediate feedback, you can discuss it here . Create an issue branch for your local work git checkout -b issue/$ISSUE-NUMBER . Do your magic here. Ensure your code matches the HOPE-8 Coding Standard used by the project. Submit a pull request to the main project repository via GitHub. Thanks for the contribution! It will quickly get reviewed, and, once accepted, will result in your name being added to the acknowledgments list :).","title":"Making a contribution"},{"location":"docs/contributing/1.-contributing-guide/#thank-you","text":"I can not tell you how thankful I am for the hard work done by hypothesis-auto contributors like you . Thank you! ~Timothy Crosley","title":"Thank you!"},{"location":"docs/contributing/2.-coding-standard/","text":"HOPE 8 -- Style Guide for Hug Code HOPE: 8 Title: Style Guide for Hug Code Author(s): Timothy Crosley timothy.crosley@gmail.com Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019 Introduction This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference. PEP 8 Foundation All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines. Line Length Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters. Descriptive Variable names Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand. Adding new modules New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible. Automated Code Cleaners All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place. Automated Code Linting All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"2. Coding Standard"},{"location":"docs/contributing/2.-coding-standard/#hope-8-style-guide-for-hug-code","text":"HOPE: 8 Title: Style Guide for Hug Code Author(s): Timothy Crosley timothy.crosley@gmail.com Status: Active Type: Process Created: 19-May-2019 Updated: 17-August-2019","title":"HOPE 8 -- Style Guide for Hug Code"},{"location":"docs/contributing/2.-coding-standard/#introduction","text":"This document gives coding conventions for the Hug code comprising the Hug core as well as all official interfaces, extensions, and plugins for the framework. Optionally, projects that use Hug are encouraged to follow this HOPE and link to it as a reference.","title":"Introduction"},{"location":"docs/contributing/2.-coding-standard/#pep-8-foundation","text":"All guidelines in this document are in addition to those defined in Python's PEP 8 and PEP 257 guidelines.","title":"PEP 8 Foundation"},{"location":"docs/contributing/2.-coding-standard/#line-length","text":"Too short of lines discourage descriptive variable names where they otherwise make sense. Too long of lines reduce overall readability and make it hard to compare 2 files side by side. There is no perfect number: but for Hug, we've decided to cap the lines at 100 characters.","title":"Line Length"},{"location":"docs/contributing/2.-coding-standard/#descriptive-variable-names","text":"Naming things is hard. Hug has a few strict guidelines on the usage of variable names, which hopefully will reduce some of the guesswork: - No one character variable names. - Except for x, y, and z as coordinates. - It's not okay to override built-in functions. - Except for id . Guido himself thought that shouldn't have been moved to the system module. It's too commonly used, and alternatives feel very artificial. - Avoid Acronyms, Abbreviations, or any other short forms - unless they are almost universally understand.","title":"Descriptive Variable names"},{"location":"docs/contributing/2.-coding-standard/#adding-new-modules","text":"New modules added to the a project that follows the HOPE-8 standard should all live directly within the base PROJECT_NAME/ directory without nesting. If the modules are meant only for internal use within the project, they should be prefixed with a leading underscore. For example, def _internal_function. Modules should contain a docstring at the top that gives a general explanation of the purpose and then restates the project's use of the MIT license. There should be a tests/test_$MODULE_NAME.py file created to correspond to every new module that contains test coverage for the module. Ideally, tests should be 1:1 (one test object per code object, one test method per code method) to the extent cleanly possible.","title":"Adding new modules"},{"location":"docs/contributing/2.-coding-standard/#automated-code-cleaners","text":"All code submitted to Hug should be formatted using Black and isort. Black should be run with the line length set to 100, and isort with Black compatible settings in place.","title":"Automated Code Cleaners"},{"location":"docs/contributing/2.-coding-standard/#automated-code-linting","text":"All code submitted to hug should run through the following tools: Black and isort verification. Flake8 flake8-bugbear Bandit pep8-naming vulture safety","title":"Automated Code Linting"},{"location":"docs/contributing/3.-code-of-conduct/","text":"HOPE 11 -- Code of Conduct HOPE: 11 Title: Code of Conduct Author(s): Timothy Crosley timothy.crosley@gmail.com Status: Active Type: Process Created: 17-August-2019 Updated: 17-August-2019 Abstract Defines the Code of Conduct for Hug and all related projects. Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting timothy.crosley@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. Confidentiality will be maintained with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the [Contributor Covenant][https://www.contributor-covenant.org], version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"3. Code Of Conduct"},{"location":"docs/contributing/3.-code-of-conduct/#hope-11-code-of-conduct","text":"HOPE: 11 Title: Code of Conduct Author(s): Timothy Crosley timothy.crosley@gmail.com Status: Active Type: Process Created: 17-August-2019 Updated: 17-August-2019","title":"HOPE 11 -- Code of Conduct"},{"location":"docs/contributing/3.-code-of-conduct/#abstract","text":"Defines the Code of Conduct for Hug and all related projects.","title":"Abstract"},{"location":"docs/contributing/3.-code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"docs/contributing/3.-code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"docs/contributing/3.-code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"docs/contributing/3.-code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"docs/contributing/3.-code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting timothy.crosley@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. Confidentiality will be maintained with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"docs/contributing/3.-code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the [Contributor Covenant][https://www.contributor-covenant.org], version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"docs/contributing/4.-acknowledgements/","text":"Contributors Core Developers Timothy Edmund Crosley (@timothycrosley) Notable Bug Reporters - Code Contributors Christian Clauss (@cclauss) Documenters Sam Havens (@sam-qordoba) Hugo van Kemenade (@hugovk) A sincere thanks to everyone who helps make hypothesis-auto into a great Python3 project! ~Timothy Crosley","title":"4. Acknowledgements"},{"location":"docs/contributing/4.-acknowledgements/#contributors","text":"","title":"Contributors"},{"location":"docs/contributing/4.-acknowledgements/#core-developers","text":"Timothy Edmund Crosley (@timothycrosley)","title":"Core Developers"},{"location":"docs/contributing/4.-acknowledgements/#notable-bug-reporters","text":"-","title":"Notable Bug Reporters"},{"location":"docs/contributing/4.-acknowledgements/#code-contributors","text":"Christian Clauss (@cclauss)","title":"Code Contributors"},{"location":"docs/contributing/4.-acknowledgements/#documenters","text":"Sam Havens (@sam-qordoba) Hugo van Kemenade (@hugovk) A sincere thanks to everyone who helps make hypothesis-auto into a great Python3 project! ~Timothy Crosley","title":"Documenters"},{"location":"reference/hypothesis_auto/","text":"Module hypothesis_auto View Source from hypothesis_auto.tester import ( Scenario , auto_parameters , auto_test , auto_test_cases , auto_test_module , ) try : from hypothesis_auto.pytest import auto_pytest , auto_pytest_magic except ImportError : # pragma: no cover pass __version__ = \"1.1.1\" Sub-modules hypothesis_auto.pytest hypothesis_auto.tester","title":"Index"},{"location":"reference/hypothesis_auto/#module-hypothesis_auto","text":"View Source from hypothesis_auto.tester import ( Scenario , auto_parameters , auto_test , auto_test_cases , auto_test_module , ) try : from hypothesis_auto.pytest import auto_pytest , auto_pytest_magic except ImportError : # pragma: no cover pass __version__ = \"1.1.1\"","title":"Module hypothesis_auto"},{"location":"reference/hypothesis_auto/#sub-modules","text":"hypothesis_auto.pytest hypothesis_auto.tester","title":"Sub-modules"},{"location":"reference/hypothesis_auto/pytest/","text":"Module hypothesis_auto.pytest View Source import inspect from typing import Any , Callable , Optional , Tuple , Union from uuid import uuid4 import pytest from hypothesis_auto.tester import Scenario , auto_test_cases def auto_pytest ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\"A decorator that marks a parameterized pytest function passing along a callable test case. The function should take a `test_case` parameter. By default auto_pytest uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_runs_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function(number_1: int, number_2: int) -> int: return number_1 + number_2 @auto_pytest(my_function) def test_auto_pytest(test_case): test_case() ----- \"\"\" return pytest . mark . parametrize ( \"test_case\" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ), ) def auto_pytest_magic ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\"A convenience function that builds a new test function inside the calling module and passes into it test cases using the `auto_pytest` decorator. The least effort and most magical way to integrate with pytest. By default auto_pytest_magic uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_runs_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function(number_1: int, number_2: int) -> int: return number_1 + number_2 auto_pytest_magic(my_function) \"\"\" called_from = inspect . stack ()[ 1 ] module = inspect . getmodule ( called_from [ 0 ]) def test_function ( test_case ): test_case () uuid = str ( uuid4 ()) . replace ( \"-\" , \"\" ) test_function . __name__ = f \"test_auto_{auto_function_.__name__}_{uuid}\" setattr ( module , test_function . __name__ , test_function ) pytest . mark . parametrize ( \"test_case\" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ), )( test_function ) Functions auto_pytest def auto_pytest ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> None A decorator that marks a parameterized pytest function passing along a callable test case. The function should take a test_case parameter. By default auto_pytest uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_runs_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 @ auto_pytest ( my_function ) def test_auto_pytest ( test_case ) : test_case () View Source def auto_pytest ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\" A decorator that marks a parameterized pytest function passing along a callable test case. The function should take a ` test_case ` parameter . By default auto_pytest uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_runs_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . Example : def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 @ auto_pytest ( my_function ) def test_auto_pytest ( test_case ) : test_case () ----- \"\"\" return pytest . mark . parametrize ( \" test_case \" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ) , ) auto_pytest_magic def auto_pytest_magic ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> None A convenience function that builds a new test function inside the calling module and passes into it test cases using the auto_pytest decorator. The least effort and most magical way to integrate with pytest. By default auto_pytest_magic uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_runs_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 auto_pytest_magic ( my_function ) View Source def auto_pytest_magic ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\" A convenience function that builds a new test function inside the calling module and passes into it test cases using the ` auto_pytest ` decorator . The least effort and most magical way to integrate with pytest . By default auto_pytest_magic uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_runs_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . Example : def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 auto_pytest_magic ( my_function ) \"\"\" called_from = inspect . stack () [ 1 ] module = inspect . getmodule ( called_from [ 0 ] ) def test_function ( test_case ) : test_case () uuid = str ( uuid4 ()) . replace ( \" - \" , \"\" ) test_function . __name__ = f \" test_auto_{auto_function_.__name__}_{uuid} \" setattr ( module , test_function . __name__ , test_function ) pytest . mark . parametrize ( \" test_case \" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ) , )( test_function )","title":"Pytest"},{"location":"reference/hypothesis_auto/pytest/#module-hypothesis_autopytest","text":"View Source import inspect from typing import Any , Callable , Optional , Tuple , Union from uuid import uuid4 import pytest from hypothesis_auto.tester import Scenario , auto_test_cases def auto_pytest ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\"A decorator that marks a parameterized pytest function passing along a callable test case. The function should take a `test_case` parameter. By default auto_pytest uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_runs_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function(number_1: int, number_2: int) -> int: return number_1 + number_2 @auto_pytest(my_function) def test_auto_pytest(test_case): test_case() ----- \"\"\" return pytest . mark . parametrize ( \"test_case\" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ), ) def auto_pytest_magic ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\"A convenience function that builds a new test function inside the calling module and passes into it test cases using the `auto_pytest` decorator. The least effort and most magical way to integrate with pytest. By default auto_pytest_magic uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_runs_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function(number_1: int, number_2: int) -> int: return number_1 + number_2 auto_pytest_magic(my_function) \"\"\" called_from = inspect . stack ()[ 1 ] module = inspect . getmodule ( called_from [ 0 ]) def test_function ( test_case ): test_case () uuid = str ( uuid4 ()) . replace ( \"-\" , \"\" ) test_function . __name__ = f \"test_auto_{auto_function_.__name__}_{uuid}\" setattr ( module , test_function . __name__ , test_function ) pytest . mark . parametrize ( \"test_case\" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ), )( test_function )","title":"Module hypothesis_auto.pytest"},{"location":"reference/hypothesis_auto/pytest/#functions","text":"","title":"Functions"},{"location":"reference/hypothesis_auto/pytest/#auto_pytest","text":"def auto_pytest ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> None A decorator that marks a parameterized pytest function passing along a callable test case. The function should take a test_case parameter. By default auto_pytest uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_runs_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 @ auto_pytest ( my_function ) def test_auto_pytest ( test_case ) : test_case () View Source def auto_pytest ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\" A decorator that marks a parameterized pytest function passing along a callable test case. The function should take a ` test_case ` parameter . By default auto_pytest uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_runs_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . Example : def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 @ auto_pytest ( my_function ) def test_auto_pytest ( test_case ) : test_case () ----- \"\"\" return pytest . mark . parametrize ( \" test_case \" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ) , )","title":"auto_pytest"},{"location":"reference/hypothesis_auto/pytest/#auto_pytest_magic","text":"def auto_pytest_magic ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> None A convenience function that builds a new test function inside the calling module and passes into it test cases using the auto_pytest decorator. The least effort and most magical way to integrate with pytest. By default auto_pytest_magic uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_runs_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. Example: def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 auto_pytest_magic ( my_function ) View Source def auto_pytest_magic ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs , ) -> None : \"\"\" A convenience function that builds a new test function inside the calling module and passes into it test cases using the ` auto_pytest ` decorator . The least effort and most magical way to integrate with pytest . By default auto_pytest_magic uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_runs_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . Example : def my_function ( number_1 : int , number_2 : int ) -> int : return number_1 + number_2 auto_pytest_magic ( my_function ) \"\"\" called_from = inspect . stack () [ 1 ] module = inspect . getmodule ( called_from [ 0 ] ) def test_function ( test_case ) : test_case () uuid = str ( uuid4 ()) . replace ( \" - \" , \"\" ) test_function . __name__ = f \" test_auto_{auto_function_.__name__}_{uuid} \" setattr ( module , test_function . __name__ , test_function ) pytest . mark . parametrize ( \" test_case \" , auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ) , )( test_function )","title":"auto_pytest_magic"},{"location":"reference/hypothesis_auto/tester/","text":"Module hypothesis_auto.tester View Source from inspect import isfunction , signature from types import ModuleType from typing import ( Any , Callable , Dict , Generator , List , NamedTuple , Optional , Tuple , Union , get_type_hints , ) from hypothesis.strategies import SearchStrategy , builds , just from pydantic import BaseModel class Parameters ( NamedTuple ): \"\"\"Represents the parameters meant to passed into a callable.\"\"\" args : List [ Any ] kwargs : Dict [ str , Any ] class TestCase ( NamedTuple ): \"\"\"Represents an individual auto generated test case. To run the test case simply call() it.\"\"\" parameters : Parameters test_function : Callable def __call__ ( self ) -> Any : \"\"\"Calls the given test case returning the called functions result on success or Raising an exception on error \"\"\" return self . test_function ( * self . parameters . args , ** self . parameters . kwargs ) class Scenario ( NamedTuple ): \"\"\"Represents entirety of the scenario being tested: - *args*: The auto-generated `*args` being passed into the test function. - *kwargs*: The auto-generated `**kwargs` being passed into the test function. - *result*: The result returned from calling the test function. - *test_function*: The test_function that was called as part of the test scenario. \"\"\" args : List [ Any ] kwargs : Dict [ str , Any ] result : Any test_function : Callable def _test_function ( auto_function_ : Callable , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), ) -> Callable : return_type = get_type_hints ( auto_function_ ) . get ( \"return\" , None ) return_model = None if return_type : class ReturnModel ( BaseModel ): __annotations__ = { \"returns\" : return_type } return_model = ReturnModel def test_function ( * args , ** kwargs ) -> Any : try : result = auto_function_ ( * args , ** kwargs ) except auto_allow_exceptions_ : # type: ignore return if return_model : return_model ( returns = result ) if auto_verify_ : auto_verify_ ( Scenario ( args = list ( args ), kwargs = kwargs , result = result , test_function = auto_function_ ) ) return result return test_function def auto_parameters ( auto_function_ : Callable , * args , auto_limit_ : int = 50 , ** kwargs ) -> Generator [ Parameters , None , None ]: \"\"\"Generates parameters from the given callable up to the specified limit (`auto_limit_` parameter). By default auto_parameters uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following option: - *auto_limit_*: Number of strategies combinations to run the given function against. \"\"\" strategy_args = [ arg if isinstance ( arg , SearchStrategy ) else just ( arg ) for arg in args ] strategy_kwargs = { name : value if isinstance ( value , SearchStrategy ) else just ( value ) for name , value in kwargs . items () } def pass_along_variables ( * args , ** kwargs ): return Parameters ( args = args , kwargs = kwargs ) pass_along_variables . __signature__ = signature ( auto_function_ ) # type: ignore pass_along_variables . __annotations__ = getattr ( auto_function_ , \"__annotations__\" , {}) strategy = builds ( pass_along_variables , * strategy_args , ** strategy_kwargs ) for _ in range ( auto_limit_ ): yield strategy . example () def auto_test_cases ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_limit_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> Generator [ TestCase , None , None ]: \"\"\"Generates test cases from the given callable up to the specified limit (`auto_limit_` parameter). By default auto_test_cases uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_limit_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. \"\"\" test_function = _test_function ( auto_function_ , auto_verify_ = auto_verify_ , auto_allow_exceptions_ = auto_allow_exceptions_ ) for parameters in auto_parameters ( auto_function_ , * args , auto_limit_ = auto_limit_ , ** kwargs ): yield TestCase ( parameters = parameters , test_function = test_function ) def auto_test ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> None : \"\"\"A simple utility function for hypothesis that enables fully automatic testing for a type hinted callable, including return type verification. By default auto_test uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_runs_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. \"\"\" for test_case in auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ** kwargs ): test_case () def auto_test_module ( module : ModuleType ) -> None : \"\"\"Attempts to automatically test every public function within a module. For the brave only.\"\"\" for attribute_name in dir ( module ): if not attribute_name . startswith ( \"_\" ): attribute = getattr ( module , attribute_name ) if isfunction ( attribute ): auto_test ( attribute ) Functions auto_parameters def auto_parameters ( auto_function_ : Callable , * args , auto_limit_ : int = 50 , ** kwargs ) -> Generator [ hypothesis_auto . tester . Parameters , NoneType , NoneType ] Generates parameters from the given callable up to the specified limit ( auto_limit_ parameter). By default auto_parameters uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following option: auto_limit_ : Number of strategies combinations to run the given function against. View Source def auto_parameters ( auto_function_ : Callable , * args , auto_limit_ : int = 50 , ** kwargs ) -> Generator [ Parameters , None , None ]: \"\"\" Generates parameters from the given callable up to the specified limit ( ` auto_limit_ ` parameter ) . By default auto_parameters uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following option : - * auto_limit_ * : Number of strategies combinations to run the given function against . \"\"\" strategy_args = [ arg if isinstance ( arg , SearchStrategy ) else just ( arg ) for arg in args ] strategy_kwargs = { name : value if isinstance ( value , SearchStrategy ) else just ( value ) for name , value in kwargs . items () } def pass_along_variables ( * args , ** kwargs ) : return Parameters ( args = args , kwargs = kwargs ) pass_along_variables . __signature__ = signature ( auto_function_ ) # type : ignore pass_along_variables . __annotations__ = getattr ( auto_function_ , \" __annotations__ \" , {} ) strategy = builds ( pass_along_variables , * strategy_args , ** strategy_kwargs ) for _ in range ( auto_limit_ ) : yield strategy . example () auto_test def auto_test ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> None A simple utility function for hypothesis that enables fully automatic testing for a type hinted callable, including return type verification. By default auto_test uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_runs_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. View Source def auto_test ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> None : \"\"\" A simple utility function for hypothesis that enables fully automatic testing for a type hinted callable , including return type verification . By default auto_test uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_runs_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . \"\"\" for test_case in auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ** kwargs ) : test_case () auto_test_cases def auto_test_cases ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_limit_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> Generator [ hypothesis_auto . tester . TestCase , NoneType , NoneType ] Generates test cases from the given callable up to the specified limit ( auto_limit_ parameter). By default auto_test_cases uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_limit_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. View Source def auto_test_cases ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_limit_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> Generator [ TestCase , None , None ]: \"\"\" Generates test cases from the given callable up to the specified limit ( ` auto_limit_ ` parameter ) . By default auto_test_cases uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_limit_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . \"\"\" test_function = _test_function ( auto_function_ , auto_verify_ = auto_verify_ , auto_allow_exceptions_ = auto_allow_exceptions_ ) for parameters in auto_parameters ( auto_function_ , * args , auto_limit_ = auto_limit_ , ** kwargs ) : yield TestCase ( parameters = parameters , test_function = test_function ) auto_test_module def auto_test_module ( module : module ) -> None Attempts to automatically test every public function within a module. For the brave only. View Source def auto_test_module ( module : ModuleType ) -> None : \"\"\" Attempts to automatically test every public function within a module. For the brave only. \"\"\" for attribute_name in dir ( module ) : if not attribute_name . startswith ( \" _ \" ) : attribute = getattr ( module , attribute_name ) if isfunction ( attribute ) : auto_test ( attribute ) Classes Parameters class Parameters ( / , * args , ** kwargs ) Represents the parameters meant to passed into a callable. View Source class Parameters ( NamedTuple ) : \"\"\"Represents the parameters meant to passed into a callable.\"\"\" args : List [ Any ] kwargs : Dict [ str, Any ] Ancestors (in MRO) builtins.tuple Instance variables args Alias for field number 0 kwargs Alias for field number 1 Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. Scenario class Scenario ( / , * args , ** kwargs ) Represents entirety of the scenario being tested: args : The auto-generated *args being passed into the test function. kwargs : The auto-generated **kwargs being passed into the test function. result : The result returned from calling the test function. test_function : The test_function that was called as part of the test scenario. View Source class Scenario ( NamedTuple ) : \"\"\"Represents entirety of the scenario being tested: - *args*: The auto-generated `*args` being passed into the test function. - *kwargs*: The auto-generated `**kwargs` being passed into the test function. - *result*: The result returned from calling the test function. - *test_function*: The test_function that was called as part of the test scenario. \"\"\" args : List [ Any ] kwargs : Dict [ str, Any ] result : Any test_function : Callable Ancestors (in MRO) builtins.tuple Instance variables args Alias for field number 0 kwargs Alias for field number 1 result Alias for field number 2 test_function Alias for field number 3 Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. TestCase class TestCase ( / , * args , ** kwargs ) Represents an individual auto generated test case. To run the test case simply call() it. View Source class TestCase ( NamedTuple ) : \"\"\" Represents an individual auto generated test case. To run the test case simply call() it. \"\"\" parameters : Parameters test_function : Callable def __call__ ( self ) -> Any : \"\"\" Calls the given test case returning the called functions result on success or Raising an exception on error \"\"\" return self . test_function ( * self . parameters . args , ** self . parameters . kwargs ) Ancestors (in MRO) builtins.tuple Instance variables parameters Alias for field number 0 test_function Alias for field number 1 Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"Tester"},{"location":"reference/hypothesis_auto/tester/#module-hypothesis_autotester","text":"View Source from inspect import isfunction , signature from types import ModuleType from typing import ( Any , Callable , Dict , Generator , List , NamedTuple , Optional , Tuple , Union , get_type_hints , ) from hypothesis.strategies import SearchStrategy , builds , just from pydantic import BaseModel class Parameters ( NamedTuple ): \"\"\"Represents the parameters meant to passed into a callable.\"\"\" args : List [ Any ] kwargs : Dict [ str , Any ] class TestCase ( NamedTuple ): \"\"\"Represents an individual auto generated test case. To run the test case simply call() it.\"\"\" parameters : Parameters test_function : Callable def __call__ ( self ) -> Any : \"\"\"Calls the given test case returning the called functions result on success or Raising an exception on error \"\"\" return self . test_function ( * self . parameters . args , ** self . parameters . kwargs ) class Scenario ( NamedTuple ): \"\"\"Represents entirety of the scenario being tested: - *args*: The auto-generated `*args` being passed into the test function. - *kwargs*: The auto-generated `**kwargs` being passed into the test function. - *result*: The result returned from calling the test function. - *test_function*: The test_function that was called as part of the test scenario. \"\"\" args : List [ Any ] kwargs : Dict [ str , Any ] result : Any test_function : Callable def _test_function ( auto_function_ : Callable , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), ) -> Callable : return_type = get_type_hints ( auto_function_ ) . get ( \"return\" , None ) return_model = None if return_type : class ReturnModel ( BaseModel ): __annotations__ = { \"returns\" : return_type } return_model = ReturnModel def test_function ( * args , ** kwargs ) -> Any : try : result = auto_function_ ( * args , ** kwargs ) except auto_allow_exceptions_ : # type: ignore return if return_model : return_model ( returns = result ) if auto_verify_ : auto_verify_ ( Scenario ( args = list ( args ), kwargs = kwargs , result = result , test_function = auto_function_ ) ) return result return test_function def auto_parameters ( auto_function_ : Callable , * args , auto_limit_ : int = 50 , ** kwargs ) -> Generator [ Parameters , None , None ]: \"\"\"Generates parameters from the given callable up to the specified limit (`auto_limit_` parameter). By default auto_parameters uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following option: - *auto_limit_*: Number of strategies combinations to run the given function against. \"\"\" strategy_args = [ arg if isinstance ( arg , SearchStrategy ) else just ( arg ) for arg in args ] strategy_kwargs = { name : value if isinstance ( value , SearchStrategy ) else just ( value ) for name , value in kwargs . items () } def pass_along_variables ( * args , ** kwargs ): return Parameters ( args = args , kwargs = kwargs ) pass_along_variables . __signature__ = signature ( auto_function_ ) # type: ignore pass_along_variables . __annotations__ = getattr ( auto_function_ , \"__annotations__\" , {}) strategy = builds ( pass_along_variables , * strategy_args , ** strategy_kwargs ) for _ in range ( auto_limit_ ): yield strategy . example () def auto_test_cases ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_limit_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> Generator [ TestCase , None , None ]: \"\"\"Generates test cases from the given callable up to the specified limit (`auto_limit_` parameter). By default auto_test_cases uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_limit_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. \"\"\" test_function = _test_function ( auto_function_ , auto_verify_ = auto_verify_ , auto_allow_exceptions_ = auto_allow_exceptions_ ) for parameters in auto_parameters ( auto_function_ , * args , auto_limit_ = auto_limit_ , ** kwargs ): yield TestCase ( parameters = parameters , test_function = test_function ) def auto_test ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> None : \"\"\"A simple utility function for hypothesis that enables fully automatic testing for a type hinted callable, including return type verification. By default auto_test uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding `*arg` or `**kwarg` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All `*arg` and `**kwargs` are automatically passed along to `hypothesis.strategies.builds` to enable this. Non strategies are automatically converted to strategies using `hypothesis.strategies.just`. Except for the following options: - *auto_allow_exceptions_*: A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. - *auto_runs_*: Number of strategies combinations to run the given function against. - *auto_verify_*: An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. \"\"\" for test_case in auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ** kwargs ): test_case () def auto_test_module ( module : ModuleType ) -> None : \"\"\"Attempts to automatically test every public function within a module. For the brave only.\"\"\" for attribute_name in dir ( module ): if not attribute_name . startswith ( \"_\" ): attribute = getattr ( module , attribute_name ) if isfunction ( attribute ): auto_test ( attribute )","title":"Module hypothesis_auto.tester"},{"location":"reference/hypothesis_auto/tester/#functions","text":"","title":"Functions"},{"location":"reference/hypothesis_auto/tester/#auto_parameters","text":"def auto_parameters ( auto_function_ : Callable , * args , auto_limit_ : int = 50 , ** kwargs ) -> Generator [ hypothesis_auto . tester . Parameters , NoneType , NoneType ] Generates parameters from the given callable up to the specified limit ( auto_limit_ parameter). By default auto_parameters uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following option: auto_limit_ : Number of strategies combinations to run the given function against. View Source def auto_parameters ( auto_function_ : Callable , * args , auto_limit_ : int = 50 , ** kwargs ) -> Generator [ Parameters , None , None ]: \"\"\" Generates parameters from the given callable up to the specified limit ( ` auto_limit_ ` parameter ) . By default auto_parameters uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following option : - * auto_limit_ * : Number of strategies combinations to run the given function against . \"\"\" strategy_args = [ arg if isinstance ( arg , SearchStrategy ) else just ( arg ) for arg in args ] strategy_kwargs = { name : value if isinstance ( value , SearchStrategy ) else just ( value ) for name , value in kwargs . items () } def pass_along_variables ( * args , ** kwargs ) : return Parameters ( args = args , kwargs = kwargs ) pass_along_variables . __signature__ = signature ( auto_function_ ) # type : ignore pass_along_variables . __annotations__ = getattr ( auto_function_ , \" __annotations__ \" , {} ) strategy = builds ( pass_along_variables , * strategy_args , ** strategy_kwargs ) for _ in range ( auto_limit_ ) : yield strategy . example ()","title":"auto_parameters"},{"location":"reference/hypothesis_auto/tester/#auto_test","text":"def auto_test ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_runs_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> None A simple utility function for hypothesis that enables fully automatic testing for a type hinted callable, including return type verification. By default auto_test uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_runs_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. View Source def auto_test ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_runs_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> None : \"\"\" A simple utility function for hypothesis that enables fully automatic testing for a type hinted callable , including return type verification . By default auto_test uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_runs_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . \"\"\" for test_case in auto_test_cases ( auto_function_ , * args , auto_allow_exceptions_ = auto_allow_exceptions_ , auto_limit_ = auto_runs_ , auto_verify_ = auto_verify_ , ** kwargs ) : test_case ()","title":"auto_test"},{"location":"reference/hypothesis_auto/tester/#auto_test_cases","text":"def auto_test_cases ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = (), auto_limit_ : int = 50 , auto_verify_ : Union [ Callable [[ hypothesis_auto . tester . Scenario ], Any ], NoneType ] = None , ** kwargs ) -> Generator [ hypothesis_auto . tester . TestCase , NoneType , NoneType ] Generates test cases from the given callable up to the specified limit ( auto_limit_ parameter). By default auto_test_cases uses type annotations to automatically decide on strategies via the hypothesis builds strategy. You can override individual strategies by passing them in under the corresponding *arg or **kwarg OR you can pass in specific values that must be used for certain parameters while letting others be auto generated. All *arg and **kwargs are automatically passed along to hypothesis.strategies.builds to enable this. Non strategies are automatically converted to strategies using hypothesis.strategies.just . Except for the following options: auto_allow_exceptions_ : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error. auto_limit_ : Number of strategies combinations to run the given function against. auto_verify_ : An optional callback function that will be called to allow custom verification of the functions return value. The callback function should raise an AssertionError if the return value does not match expectations. View Source def auto_test_cases ( auto_function_ : Callable , * args , auto_allow_exceptions_ : Union [ Tuple [ BaseException ], Tuple ] = () , auto_limit_ : int = 50 , auto_verify_ : Optional [ Callable [[ Scenario ], Any ]] = None , ** kwargs ) -> Generator [ TestCase , None , None ]: \"\"\" Generates test cases from the given callable up to the specified limit ( ` auto_limit_ ` parameter ) . By default auto_test_cases uses type annotations to automatically decide on strategies via the hypothesis builds strategy . You can override individual strategies by passing them in under the corresponding ` * arg ` or ` ** kwarg ` OR you can pass in specific values that must be used for certain parameters while letting others be auto generated . All ` * arg ` and ` ** kwargs ` are automatically passed along to ` hypothesis . strategies . builds ` to enable this . Non strategies are automatically converted to strategies using ` hypothesis . strategies . just `. Except for the following options : - * auto_allow_exceptions_ * : A tuple of exceptions that are acceptable for the function to raise and will no be considered a test error . - * auto_limit_ * : Number of strategies combinations to run the given function against . - * auto_verify_ * : An optional callback function that will be called to allow custom verification of the functions return value . The callback function should raise an AssertionError if the return value does not match expectations . \"\"\" test_function = _test_function ( auto_function_ , auto_verify_ = auto_verify_ , auto_allow_exceptions_ = auto_allow_exceptions_ ) for parameters in auto_parameters ( auto_function_ , * args , auto_limit_ = auto_limit_ , ** kwargs ) : yield TestCase ( parameters = parameters , test_function = test_function )","title":"auto_test_cases"},{"location":"reference/hypothesis_auto/tester/#auto_test_module","text":"def auto_test_module ( module : module ) -> None Attempts to automatically test every public function within a module. For the brave only. View Source def auto_test_module ( module : ModuleType ) -> None : \"\"\" Attempts to automatically test every public function within a module. For the brave only. \"\"\" for attribute_name in dir ( module ) : if not attribute_name . startswith ( \" _ \" ) : attribute = getattr ( module , attribute_name ) if isfunction ( attribute ) : auto_test ( attribute )","title":"auto_test_module"},{"location":"reference/hypothesis_auto/tester/#classes","text":"","title":"Classes"},{"location":"reference/hypothesis_auto/tester/#parameters","text":"class Parameters ( / , * args , ** kwargs ) Represents the parameters meant to passed into a callable. View Source class Parameters ( NamedTuple ) : \"\"\"Represents the parameters meant to passed into a callable.\"\"\" args : List [ Any ] kwargs : Dict [ str, Any ]","title":"Parameters"},{"location":"reference/hypothesis_auto/tester/#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/hypothesis_auto/tester/#instance-variables","text":"args Alias for field number 0 kwargs Alias for field number 1","title":"Instance variables"},{"location":"reference/hypothesis_auto/tester/#methods","text":"","title":"Methods"},{"location":"reference/hypothesis_auto/tester/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/hypothesis_auto/tester/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/hypothesis_auto/tester/#scenario","text":"class Scenario ( / , * args , ** kwargs ) Represents entirety of the scenario being tested: args : The auto-generated *args being passed into the test function. kwargs : The auto-generated **kwargs being passed into the test function. result : The result returned from calling the test function. test_function : The test_function that was called as part of the test scenario. View Source class Scenario ( NamedTuple ) : \"\"\"Represents entirety of the scenario being tested: - *args*: The auto-generated `*args` being passed into the test function. - *kwargs*: The auto-generated `**kwargs` being passed into the test function. - *result*: The result returned from calling the test function. - *test_function*: The test_function that was called as part of the test scenario. \"\"\" args : List [ Any ] kwargs : Dict [ str, Any ] result : Any test_function : Callable","title":"Scenario"},{"location":"reference/hypothesis_auto/tester/#ancestors-in-mro_1","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/hypothesis_auto/tester/#instance-variables_1","text":"args Alias for field number 0 kwargs Alias for field number 1 result Alias for field number 2 test_function Alias for field number 3","title":"Instance variables"},{"location":"reference/hypothesis_auto/tester/#methods_1","text":"","title":"Methods"},{"location":"reference/hypothesis_auto/tester/#count_1","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/hypothesis_auto/tester/#index_1","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/hypothesis_auto/tester/#testcase","text":"class TestCase ( / , * args , ** kwargs ) Represents an individual auto generated test case. To run the test case simply call() it. View Source class TestCase ( NamedTuple ) : \"\"\" Represents an individual auto generated test case. To run the test case simply call() it. \"\"\" parameters : Parameters test_function : Callable def __call__ ( self ) -> Any : \"\"\" Calls the given test case returning the called functions result on success or Raising an exception on error \"\"\" return self . test_function ( * self . parameters . args , ** self . parameters . kwargs )","title":"TestCase"},{"location":"reference/hypothesis_auto/tester/#ancestors-in-mro_2","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/hypothesis_auto/tester/#instance-variables_2","text":"parameters Alias for field number 0 test_function Alias for field number 1","title":"Instance variables"},{"location":"reference/hypothesis_auto/tester/#methods_2","text":"","title":"Methods"},{"location":"reference/hypothesis_auto/tester/#count_2","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/hypothesis_auto/tester/#index_2","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"}]}